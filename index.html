<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Evaluating Large Language Models in STEM Education: Towards a Holistic Assessment Framework</title>
    <!-- Include the Marked library -->
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    <style>
        /* Apple Style CSS */

        /* Font and Typography */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;
            color: #1d1d1f; /* Apple's text color */
            background-color: #fff;
            margin: 0;
            padding: 20px;
            line-height: 1.6;
        }

        h1, h2, h3, h4, h5, h6 {
            font-weight: 600;
            color: #000;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
        }

        h1 {
            font-size: 2.6em;
        }

        h2 {
            font-size: 2em;
        }

        h3 {
            font-size: 1.75em;
        }

        h4 {
            font-size: 1.5em;
        }

        h5 {
            font-size: 1.25em;
        }

        h6 {
            font-size: 1em;
        }

        p {
            margin: 1em 0;
            font-size: 1em;
        }

        /* Links */
        a {
            color: #007aff; /* Apple's link color */
            text-decoration: none;
        }

        a:hover {
            text-decoration: underline;
        }

        /* Lists */
        ul, ol {
            margin-left: 1.5em;
            margin-bottom: 1em;
        }

        li {
            margin-bottom: 0.5em;
        }

        /* Code Blocks */
        pre {
            background-color: #f5f5f7;
            padding: 16px;
            overflow-x: auto;
            border-radius: 8px;
            font-size: 0.95em;
        }

        code {
            background-color: #f5f5f7;
            padding: 2px 4px;
            border-radius: 4px;
            font-family: Menlo, Monaco, Consolas, "Courier New", monospace;
            font-size: 0.95em;
        }

        /* Blockquotes */
        blockquote {
            border-left: 4px solid #d1d1d6;
            padding-left: 1em;
            color: #6e6e73;
            margin: 1em 0;
        }

        /* Tables */
        table {
            border-collapse: collapse;
            width: 100%;
            margin-bottom: 1em;
        }

        th, td {
            border: 1px solid #d1d1d6;
            padding: 12px;
            text-align: left;
        }

        th {
            background-color: #f5f5f7;
            font-weight: 600;
        }

        /* Images */
        img {
            max-width: 100%;
            display: block;
            margin: 1em 0;
        }

        /* Horizontal Rule */
        hr {
            border: none;
            border-top: 1px solid #d1d1d6;
            margin: 2em 0;
        }

        /* Header */
        #header {
            border-bottom: 1px solid #d1d1d6;
            padding-bottom: 20px;
            margin-bottom: 40px;
        }

        /* Footer */
        #footer {
            border-top: 1px solid #d1d1d6;
            padding-top: 20px;
            margin-top: 40px;
            font-size: 0.9em;
            color: #6e6e73;
            text-align: center;
        }
    </style>
</head>
<body>
    <div id="content"></div>

    <div id="footer">
        2024 Patrick Fan. Project assignment from BlueDot Impact AI Alignment Course, Cohort 37.
    </div>

    <script>
        // Embed your Markdown content here
        const markdown = `
# Evaluating LLMs in STEM Education
*This research blog is still under review.*

*By Patrick Fan, project assignment from BlueDot Impact AI Alignment Course, Cohort 37.*

### Introduction

"Imagine a world where AI tutors students in complex mathematical concepts, making personalized education accessible to millions. Are we there yet?" The rapid evolution of large language models (LLMs) is bringing us closer to this vision. LLMs, trained on massive datasets of text and code, are transforming various fields, and their potential impact on education is particularly profound. From AI-powered tutors to automated essay graders, these models are already reshaping how students learn and teachers teach.

At their core, LLMs are statistical models that predict the probability of word sequences based on patterns observed in their training data. This enables them to generate contextually relevant, grammatically correct text, making them incredibly versatile for educational applications. LLMs can explain concepts, answer questions, generate practice problems, and even engage in dialogue that mimics human conversation, opening the door to dynamic and interactive learning experiences.

However, the successful integration of LLMs into education depends not only on their capabilities but also on how we evaluate them. Traditional metrics, such as accuracy, are important but insufficient for capturing the complexities of teaching and learning. To truly assess the effectiveness of LLMs in education, we must look at a broader set of criteria: coherence, relevance, pedagogical soundness, and safety, among others. For example, how well an LLM explains a scientific concept to a primary school student is just as critical as its ability to solve a math equation.

Moreover, the use of LLMs in education raises essential questions about bias, ethics, and student impact. It is not just about what these models can do, but how responsibly they are developed and deployed. A robust, comprehensive evaluation framework is vital to ensure that LLMs enhance learning in ways that are not only effective but also safe and equitable.

### Challenges in Evaluating LLMs for Education

Evaluating Large Language Models (LLMs) in educational settings presents unique challenges that go beyond traditional AI assessment metrics. It’s not enough for an LLM to simply provide correct answers; it must also demonstrate an understanding of pedagogical principles and effectively engage students. Let’s explore some of the key challenges in this context:

**Complex Nature of Assessment:**

Educational assessment is much more nuanced than factual accuracy. While producing correct answers is important, it doesn’t fully capture the essence of effective teaching and learning. For instance, an LLM might successfully solve a math problem, yet struggle to explain the underlying concepts in an age-appropriate or pedagogically sound way. A primary school student may need step-by-step guidance to understand fractions, not just a correct answer. This highlights the need for evaluation frameworks that assess an LLM's reasoning, problem-solving strategies, and ability to convey concepts clearly and appropriately for the learner’s developmental stage.

**Subjective Aspects:**

Many key aspects of teaching—such as coherence, relevance, and pedagogical soundness—are subjective by nature. What constitutes a clear and engaging explanation varies depending on the student's learning style, prior knowledge, and even cultural context. For example, while an LLM might generate a factually correct explanation, it could still be incoherent or too complex for a young learner. This makes it critical to incorporate human judgment and diverse perspectives in the evaluation process. Assessing subjective factors requires considering how well the LLM aligns with different educational needs, ensuring that the content resonates across various learning environments.

**Static vs Dynamic Evaluation:**

Traditional AI evaluation often relies on static datasets and pre-defined tasks. However, real-world education is a dynamic and evolving process, where students’ understanding deepens through ongoing interactions with teachers and peers. Imagine an interactive learning scenario where an LLM is tutoring a student on fractions. An effective LLM should be able to adjust its teaching approach based on the student’s responses—offering additional support for misunderstood concepts or introducing more complex material as the student progresses. Capturing this dynamic interplay between the LLM and the learner requires moving beyond static evaluations toward more interactive and real-time assessments that reflect the ongoing learning journey.

**Explainability and Bias:**

One of the critical challenges with LLMs, especially those based on large-scale deep learning, is their "black box" nature, where the reasoning behind their outputs is opaque. This lack of explainability can be problematic in educational settings, particularly when an LLM provides an incorrect or biased response. Educators need to trust these systems and understand how they arrive at certain conclusions. For example, if an LLM consistently demonstrates biases that disadvantage certain student groups or perpetuates harmful stereotypes, it risks undermining educational fairness and equity. Tackling this challenge requires not only improving the transparency of LLMs’ decision-making processes but also ensuring that these systems are trained on diverse and inclusive datasets that promote unbiased learning outcomes.

### Key Evaluation Criteria for LLMs in Education: Moving Beyond Accuracy

When assessing the effectiveness of LLMs in educational contexts—especially for primary school STEM learning—it is essential to adopt a comprehensive set of criteria that go beyond mere factual accuracy. These criteria can be grouped into categories such as knowledge, reasoning, communication, and ethics to ensure a holistic evaluation.

### **1. Knowledge:**

- **Accuracy and Completeness:**  
    Factual accuracy is a fundamental requirement, but evaluating the completeness and depth of the LLM's knowledge is equally important. An LLM should not only solve a math problem correctly but also demonstrate a deep understanding of the underlying principles and their interconnections.  
    **Example:** In a math lesson, beyond providing the correct answer, an LLM should be able to explain how different mathematical concepts, like geometry and algebra, are interrelated.
    
- **Currency:**  
    In rapidly evolving fields like science and technology, it is vital to assess whether an LLM can provide up-to-date information and anticipate future developments.  
    **Example:** In astronomy, an LLM should be able to explain recent discoveries, like new exoplanets or changes in our understanding of black holes, ensuring that students learn current information.
    

### **2. Reasoning:**

- **Problem-Solving:**  
    Assessing an LLM’s problem-solving abilities involves determining its capacity to apply knowledge in novel situations and generate creative solutions.  
    **Example:** In geometry, if asked to calculate the area of an irregular shape, an effective LLM could guide the student through strategies such as dividing the shape into simpler figures or using coordinate geometry, adapting its approach as the student progresses.
    
- **Critical Thinking:**  
    Critical thinking is about analyzing information, identifying assumptions, and constructing logical arguments.  
    **Example:** In a lesson on climate change, an LLM could present multiple viewpoints, prompting students to evaluate evidence and form their own conclusions based on scientific reasoning.
    
- **Pedagogical Soundness:**  
    This criterion evaluates whether the LLM can present information in a manner aligned with effective teaching practices.  
    **Example:** When teaching fractions, an LLM could use scaffolding techniques—breaking down the concept into simpler steps to ensure the student fully understands before advancing to more complex tasks.
    

### **3. Communication:**

- **Clarity and Coherence:**  
    LLMs should communicate information in a clear and structured manner that is easy for students to comprehend.  
    **Example:** In explaining photosynthesis, an LLM should avoid jargon and provide a step-by-step, age-appropriate explanation that a young learner can easily follow.
    
- **Relevance and Engagement:**  
    The LLM should adapt its communication style and content to the learner’s needs and interests to enhance engagement.  
    **Example:** To make STEM topics more engaging for young students, an LLM could use interactive simulations or tell a story about a scientist making a discovery to contextualize complex ideas like electricity or gravity.
    
- **Interactive Learning:**  
    The ability to engage in dynamic, two-way communication is vital for personalized learning.  
    **Example:** Conversational AI, powered by LLMs, can simulate student-teacher dialogue, asking follow-up questions, offering feedback, and adjusting explanations based on the student's understanding.
    

### **4. Ethics:**

- **Fairness and Bias:**  
    Evaluating LLMs for potential biases is crucial to ensure they don’t perpetuate or exacerbate inequalities in education.  
    **Example:** An LLM used for grading essays should be tested for biases related to gender, race, or socioeconomic background, ensuring fairness in assessments.
    
- **Privacy and Security:**  
    As LLMs often require access to personal data for personalization, it is critical to ensure that student data is handled ethically and securely.
    
- **Transparency and Explainability:**  
    LLMs should be developed and deployed in ways that are transparent, allowing educators to understand the system's decisions and fostering trust.  
    **Example:** Teachers should be able to explain how the LLM arrived at a particular answer, especially in cases where the student’s grade or feedback is affected.

By applying these comprehensive criteria, we can move beyond simplistic measures of accuracy to ensure LLMs are developed in ways that genuinely benefit learners. A well-rounded evaluation will help ensure that LLMs not only provide correct answers but also enhance critical thinking, foster engagement, and uphold ethical standards in education.

### Existing Evaluation Frameworks and Methods

#### Critical Review: Limitations of General-Purpose Benchmarks

General-purpose benchmarks often fall short when evaluating LLMs for educational applications, particularly in STEM fields. These benchmarks tend to focus on tasks that do not reflect the nuanced requirements of teaching and learning. Some common limitations include:

- **Language Proficiency:**  
    Many benchmarks assess an LLM's ability to perform well on standardized language tests, which emphasize grammar, vocabulary, and reading comprehension. While valuable, these skills do not necessarily translate to effective STEM instruction, where clear communication of complex concepts and problem-solving are critical.
    
- **Multiple-Choice Question Answering:**  
    Benchmarks that rely heavily on multiple-choice questions favor rote memorization rather than assessing deeper conceptual understanding. They fail to test an LLM’s ability to explain ideas, solve open-ended problems, or engage in the kind of dynamic reasoning that is essential for STEM education.
    
- **Rote Tasks:**  
    Many evaluations focus on tasks like grammar correction or basic question-answering based on factual recall. These tasks do not capture the complexity of teaching subjects like math or science to young students, who need engaging explanations, adaptive instruction, and the ability to connect ideas meaningfully.
    

#### Dynamic Evaluation & Adversarial Prompting

**Dynamic Evaluation**

Dynamic evaluation recognizes that learning is an evolving, interactive process, rather than a static one. This approach is particularly important in educational settings, where a student's understanding deepens through ongoing interactions. Here are some concrete examples of how dynamic evaluation can assess an LLM:

- **Adjusting to Unexpected Answers:**  
    In a dynamic setting, an LLM should be evaluated on how well it handles a student’s unexpected or incorrect responses. For example, when tutoring a student in math, the LLM should be able to recognize a mistake and respond appropriately—whether by providing hints, breaking the problem into simpler steps, or offering encouragement.
    
- **Tailoring Explanations:**  
    As a student progresses, the LLM should adapt its explanations to match the student’s growing understanding. Early in a lesson, the LLM might provide more detailed guidance, but as the student masters the material, it should shift to more concise explanations. A dynamic evaluation would assess how effectively the LLM adjusts its teaching strategies in real-time.
    
- **Personalizing Learning Paths:**  
    A dynamic evaluation could also measure how well an LLM personalizes learning paths based on individual progress. For instance, the model should be able to recommend additional practice problems, suggest alternative resources, or adjust the pace of instruction based on the student's needs and achievements.
    

**Adversarial Prompting**

Adversarial prompting tests the robustness and reliability of LLMs by deliberately crafting tricky or ambiguous questions. This technique helps reveal an LLM’s weaknesses or biases. Here are a few examples of how adversarial prompting can be applied:

- **Identifying Conceptual Misunderstandings:**  
    An LLM might be presented with a math problem designed to trigger common misconceptions. For instance, a prompt might ask the LLM to explain why a particular solution method is incorrect, testing its ability to recognize and correct flawed reasoning.
    
- **Handling Ambiguity:**  
    In real-world education, students often ask questions that are open to interpretation or lack sufficient context. Adversarial prompting could test an LLM’s ability to manage ambiguous or incomplete queries. For example, when faced with an unclear question about a scientific concept, the LLM should seek clarification or offer different interpretations.
    
- **Detecting Biases:**  
    Adversarial prompts can also help identify biases in LLMs. For example, specially crafted prompts might subtly test whether the LLM favors certain demographics or reinforces stereotypes in its explanations or problem-solving strategies. This ensures the model is fair and inclusive in educational settings.

By employing both dynamic evaluation and adversarial prompting, we can develop a deeper understanding of an LLM's capabilities in educational contexts. These techniques go beyond static assessments and offer insights into an LLM's adaptability, robustness, and ability to handle complex reasoning, all of which are essential for effective STEM education.

### Recommendations for a More Holistic Evaluation Framework for LLMs in Education

To develop a more comprehensive framework for evaluating LLMs in primary school STEM education, we need to go beyond traditional metrics. This approach incorporates detailed rubrics, interactive scenarios, and real-world feedback, ensuring that LLMs provide not only accurate answers but also effective and ethical instruction.

### 1. Highly Detailed Rubrics

Rubrics should provide specific, measurable criteria that capture the quality of LLM outputs across knowledge, reasoning, communication, and ethics.

|**Evaluation Area**|**Description**|**Example Rubric**|
|---|---|---|
|**Knowledge**|Assess how well the LLM not only provides the correct answer but demonstrates an understanding of the process.|**Score 1:** Correct answer without explanation.  <br>**Score 2:** Basic understanding of the formula but lacks clear reasoning.  <br>**Score 3:** Step-by-step explanation with a strong grasp of underlying concepts.|
|**Reasoning**|Evaluate the LLM's ability to apply concepts to new situations and solve problems requiring critical thinking.|**Score 1:** Incorrect solution or failure to apply concepts.  <br>**Score 2:** Attempts application but with errors in logic or calculation.  <br>**Score 3:** Correctly applies concepts, demonstrating logical reasoning and accuracy.|
|**Communication**|Assess the clarity, coherence, and appropriateness of the LLM’s explanations for a student’s level of understanding.|**Score 1:** Unclear or confusing language inappropriate for the student’s age.  <br>**Score 2:** Attempts to simplify but includes complex terms.  <br>**Score 3:** Consistently uses age-appropriate, clear language with engaging examples.|
|**Ethics**|Ensure the LLM promotes inclusivity, avoids reinforcing biases, and respects student diversity.|**Score 1:** Displays bias or uses language that could disadvantage certain groups.  <br>**Score 2:** Some inclusivity, but subtle biases remain.  <br>**Score 3:** Actively uses inclusive language and diverse examples without bias.|

### 2. Interactive Testing Scenarios

Static benchmarks fail to assess how LLMs respond dynamically to student interactions. Interactive scenarios allow for testing adaptability, engagement, and pedagogical effectiveness in real-world contexts.

|**Scenario**|**Prompt**|**Evaluation Criteria**|
|---|---|---|
|**Teaching Physics Concepts**|"Imagine you are teaching a student about gravity. The student asks, ‘If I drop a feather and a bowling ball at the same time, won’t the bowling ball hit the ground first because it’s heavier?’ How would you respond?"|**Accuracy:** Factually correct explanation of gravity.  <br>**Clarity:** Clear, concise explanation for a primary school student.  <br>**Engagement:** Use of relatable examples or analogies.  <br>**Pedagogical Soundness:** Directly addresses the student’s misconception and guides them toward deeper understanding.|
|**Math Problem-Solving**|"A student solves an equation incorrectly. How would you guide the student to the correct solution?"|**Feedback Adaptability:** Ability to adjust the explanation based on the student’s error.  <br>**Step-by-Step Guidance:** Providing clear, iterative steps to correct the mistake.  <br>**Encouragement:** Motivates the student to continue learning.|

### 3. Teacher and Student Feedback

Incorporating qualitative feedback from teachers and students provides essential real-world insights into the usability, effectiveness, and engagement of LLMs in classrooms. This data, combined with rubric-based metrics, can help create a more comprehensive evaluation.

|**Feedback Category**|**Questions for Educators and Students**|**Purpose**|
|---|---|---|
|**Usability**|"How easy is the LLM to use in the classroom?"|Evaluate how seamlessly the LLM integrates into everyday classroom activities.|
|**Effectiveness**|"Do students show measurable learning improvements after using the LLM?"|Measure the LLM’s impact on student learning outcomes, such as grades and understanding.|
|**Engagement**|"Are students motivated and engaged by the LLM’s teaching style?"|Assess how well the LLM maintains student interest and adapts to different learning styles.|
|**Ethics**|"Do you have concerns about bias or the LLM’s impact on teacher-student dynamics?"|Identify potential ethical issues related to the LLM’s behavior and its broader effects on classroom relationships.|
### Interactive Testing Scenarios with Sample Prompts and Poor LLM Results

| **Scenario**                             | **Prompt**                                                                                                                                                                                                                | **Evaluation Criteria**                                                                  | **Example of Poor LLM Result**                                                                                                                                                                                                                                                                                             |
| ---------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Teaching Physics Concepts**            | "Imagine you are teaching a student about gravity. The student asks, ‘If I drop a feather and a bowling ball at the same time, won’t the bowling ball hit the ground first because it’s heavier?’ How would you respond?" | - **Accuracy**  <br>- **Clarity**  <br>- **Engagement**  <br>- **Pedagogical Soundness** | **Response:** "Yes, because the bowling ball is heavier, it will fall faster."  <br>**Issue:** The explanation is factually incorrect and reinforces the student’s misconception. The LLM fails to explain air resistance and how objects fall at the same rate in a vacuum.                                               |
| **Math Problem-Solving**                 | "A student solves the equation 2x + 3 = 7 incorrectly by dividing both sides by 2 first. How would you guide the student to the correct solution?"                                                                        | - **Feedback Adaptability**  <br>- **Step-by-Step Guidance**  <br>- **Encouragement**    | **Response:** "That’s wrong, you need to try again."  <br>**Issue:** The response is dismissive and does not provide any guidance on how to approach the solution. The LLM fails to help the student understand their mistake or provide steps to fix it.                                                                  |
| **Teaching Basic Chemistry**             | "A student asks why water is a liquid at room temperature while oxygen is a gas. How would you explain the difference in their states?"                                                                                   | - **Clarity**  <br>- **Engagement**  <br>- **Pedagogical Soundness**                     | **Response:** "Water has hydrogen bonds and oxygen doesn’t, so water stays liquid."  <br>**Issue:** The response is overly simplistic, vague, and lacks engagement. It does not provide sufficient explanation of molecular structure and interaction.                                                                     |
| **Word Problem Comprehension in Math**   | "The word problem is: ‘A train travels 60 miles in 2 hours. How far will it travel in 5 hours at the same speed?’ The student answers 100 miles. How would you respond to help the student understand their error?"       | - **Problem-Solving**  <br>- **Clarity**  <br>- **Engagement**                           | **Response:** "That’s wrong, the correct answer is 150 miles."  <br>**Issue:** The LLM does not explain the reasoning behind the correct answer. It doesn’t walk the student through the calculation or address the student’s mistake with empathy or clarity.                                                             |
| **Addressing Ethical Issues in History** | "A student is writing a report on the Industrial Revolution and says, ‘All technological advances are good for society, right?’ How would you respond to encourage critical thinking?"                                    | - **Ethics**  <br>- **Critical Thinking**  <br>- **Bias Awareness**                      | **Response:** "Yes, technological advances always lead to progress and better living standards."  <br>**Issue:** The response lacks critical thinking and presents a biased, one-sided view. It misses the opportunity to prompt the student to consider both the positive and negative impacts of technological advances. |
| **Language Arts: Analyzing Poetry**      | "A student asks, ‘What does the phrase “time’s winged chariot” in the poem mean?’ How would you explain this to them?"                                                                                                    | - **Clarity**  <br>- **Reasoning**  <br>- **Engagement**                                 | **Response:** "It just means time is fast."  <br>**Issue:** The explanation is overly simplistic and lacks depth. The LLM misses the opportunity to engage the student by exploring metaphors or deeper literary themes, which would enhance their understanding.                                                          |
| **Teaching Ethics in Computing**         | "A student asks, ‘Is it okay to use other people’s code in my project if I find it online?’ How would you guide the student?"                                                                                             | - **Ethical Awareness**  <br>- **Clarity**  <br>- **Pedagogical Soundness**              | **Response:** "Yes, as long as you modify it a little bit."  <br>**Issue:** The response promotes unethical behavior by condoning plagiarism. It fails to explain the importance of respecting intellectual property, licensing, and the ethics of using open-source code.                                                 |
| **Science Experiment Design**            | "A student is planning a science experiment to test how sunlight affects plant growth. The student says, ‘I’ll put one plant in the sun and one in a dark room.’ How would you help improve their experimental design?"   | - **Critical Thinking**  <br>- **Reasoning**  <br>- **Engagement**                       | **Response:** "That’s fine, you’ll see the difference in growth."  <br>**Issue:** The LLM fails to address the lack of control variables in the experiment, such as soil, water, and temperature. It misses the opportunity to teach the student about scientific rigor and experimental design.                           |

### Analysis of Poor LLM Results:

1. **Accuracy Failures**: In cases like the physics example, the LLM provides factually incorrect information, which could mislead the student and deepen their misconceptions.
2. **Lack of Critical Thinking**: The Industrial Revolution example shows how the LLM can miss opportunities to encourage deeper analysis and critical thinking, resulting in oversimplified, biased responses.
3. **Lack of Engagement**: Responses like "That’s wrong" fail to engage students and guide them toward understanding their mistakes. These examples show how an LLM might provide correct answers without fostering deeper learning.
4. **Failure to Adapt**: In the math problem-solving scenario, the LLM doesn’t adapt to the student's mistake and offer tailored guidance, which is crucial for personalized learning.
5. **Ethical Lapses**: In the computing ethics example, the LLM’s response encourages unethical behavior, demonstrating the importance of embedding ethical reasoning into LLM interactions.

## Conclusion

### Reiterate Key Takeaways

LLMs have the potential to revolutionize STEM education, offering personalized learning, real-time feedback, and adaptive instruction. However, realizing this potential requires a rigorous evaluation of their ability to teach effectively, safely, and adaptively. Current benchmarks, focused on tasks like grammar correction and multiple-choice question answering, fall short of assessing the complex skills needed for effective STEM instruction—such as reasoning, communication, and ethical decision-making.

### Call to Action

To harness the full power of LLMs in STEM education, educators, AI researchers, and policymakers must work together to create more robust, domain-specific evaluation frameworks that reflect the dynamic nature of teaching and learning. Moving forward, this collaborative effort should prioritize the following:

- **Developing Highly Detailed Rubrics**: Traditional metrics such as accuracy are insufficient. We need rubrics that assess how well LLMs explain concepts, reason through problems, adapt to student understanding, and uphold ethical considerations. These rubrics should capture the nuances of STEM education and evaluate LLMs on more than just the correctness of answers.
    
- **Creating Interactive Testing Scenarios**: Real-world classroom interactions are complex, and LLMs should be evaluated based on their ability to handle dynamic learning situations. Interactive scenarios that simulate classroom environments will provide deeper insights into how well LLMs address student misconceptions, respond to unexpected questions, and engage in meaningful dialogue.
    
- **Gathering Feedback from Teachers and Students**: Quantitative metrics alone are not enough. Incorporating qualitative feedback from teachers and students through pilot programs and interviews is crucial. This real-world feedback will help identify the strengths and limitations of LLMs in actual educational settings, ensuring that these tools meet the needs of both learners and educators.



---

## References

- *PromptBench1: A unified library designed for evaluating LLMs, with features like prompt engineering, adversarial prompt attacks, and various evaluation protocols.*
- *CheckEval11: An evaluation framework that leverages checklists generated by LLMs to robustly evaluate LLM outputs.*
- *FreeEval14: A modular framework that prioritizes trustworthy and efficient LLM evaluation through features like contamination detection and meta-evaluation.*
- *OpenFactCheck18: A system focusing on the factuality evaluation of LLMs, allowing customized fact-checking and comparison with other systems.*
- *LLM Evaluation Framework for Astronomy Research29: This framework centers on evaluating how astronomy researchers utilize LLMs, particularly those augmented with retrieval capabilities*
- *LLM Evaluation for Decision-Making in Uncertain Contexts35: This framework examines LLM behavior in scenarios involving uncertainty, particularly in the context of financial decision-making*
- *S.C.O.R.E. Evaluation Framework36: Proposes a comprehensive evaluation framework for LLMs considering safety, consensus, objectivity, reproducibility, and explainability\*
- *Dr.Academy41: A benchmark designed to evaluate the question-generating capability of LLMs within an educational context*
- *LalaEval45: A human-centered evaluation framework specifically designed for domain-specific LLMs, focusing on chatbot conversations*
- *LLM-based Chatbot for Teaching English Conversation47: This study explores the development and evaluation of LLMs for teaching English conversation, particularly in EFL settings*
- *Contestable AI Framework for Essay Evaluation64: This framework introduces the concept of contestability, allowing for interactive feedback and argumentation in the automated assessment of student essays*
- *Automated Assessment of Multimodal Answer Sheets in STEM69: This source touches on the use of LLMs in evaluating complex, multimodal answers, particularly in STEM subjects*
- *CJEval70: A benchmark utilizing Chinese Junior High School exam data to assess LLM capabilities in an educational context*
- *Knowledge Tracing in Tutor-Student Dialogues73: This research introduces dialogue knowledge tracing, a novel task that leverages LLMs to analyze student learning within educational dialogues*
- *LLaMa-SciQ78: Focuses on an educational chatbot designed specifically for answering multiple-choice questions in science*
        `;

        // Parse and display the Markdown content
        const html = marked.parse(markdown);
        document.getElementById('content').innerHTML = html;
    </script>

</body>
</html>
